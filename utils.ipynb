{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8931c6f9-ae8f-47f9-aacb-dbfb951ab7be",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd567541-16c1-45d0-93e9-7830f30cd2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from kge.model import KgeModel\n",
    "from kge.util.io import load_checkpoint\n",
    "from kge.indexing import index_relation_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670963d-95c5-4f5c-9f25-6f7916840afc",
   "metadata": {},
   "source": [
    "# Lazy Getter Decorator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c41cd1e-8942-42f2-8534-accc14dbe388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_df_getter(func):\n",
    "    def get(*args, **kwargs):\n",
    "        cleaned_arg = args[0].replace('\\\\', '-').replace('.', '-').replace('/', '-')\n",
    "        path_to_file = os.path.join('temp', f'{func.__name__}-{cleaned_arg}.txt')\n",
    "        if os.path.exists(path_to_file):\n",
    "            print(f'using precalculated values from {path_to_file}')\n",
    "            return pd.read_csv(path_to_file)\n",
    "        else:\n",
    "            df = func(*args, **kwargs)\n",
    "            df.to_csv(path_to_file)\n",
    "            return df        \n",
    "    return get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3268b2-817e-4d43-9f53-f5a1f634552b",
   "metadata": {},
   "source": [
    "# Entities/ Relations <-> Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e75faea8-9159-41bf-aa8c-c474eb0d6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERI:\n",
    "\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.entities = pd.read_csv(os.path.join(dataset_path, 'entity_ids.del'), sep='\\t', names=['id', 'entity'])\n",
    "        self.relations = pd.read_csv(os.path.join(dataset_path, 'relation_ids.del'), sep='\\t', names=['id', 'relation'])\n",
    "    \n",
    "    def _get_multiple(self, getter, inputs):        \n",
    "        results = []\n",
    "        for input in inputs:\n",
    "            results.append(getter(input))\n",
    "        return results\n",
    "    \n",
    "    def get_entity_by_id(self, id):\n",
    "        return self.entities.loc[self.entities['id'] == id].iat[0,1]\n",
    "    \n",
    "    def get_entities_by_id(self, ids):\n",
    "        return self._get_multiple(self.get_entity_by_id, ids)\n",
    "    \n",
    "    def get_entity_id(self, entity):\n",
    "        if type(entity) == str and entity.isdigit():\n",
    "            entity = int(entity)\n",
    "        return self.entities.loc[self.entities['entity'] == entity].iat[0,0]\n",
    "    \n",
    "    def get_entity_ids(self, entities):\n",
    "        return self._get_multiple(self.get_entity_id, entities)\n",
    "    \n",
    "    def get_relation_by_id(self, id):\n",
    "        return self.relations.loc[self.relations['id'] == id].iat[0,1]\n",
    "    \n",
    "    def get_relations_by_id(self, ids):\n",
    "        return self._get_multiple(self.get_relation_by_id, ids)\n",
    "    \n",
    "    def get_relation_id(self, relation):\n",
    "        if relation.isdigit():\n",
    "            relation = int(relation)\n",
    "        return self.relations.loc[self.relations['relation'] == relation].iat[0,0]\n",
    "    \n",
    "    def get_relation_ids(self, relations):\n",
    "        return self._get_multiple(self.get_relation_id, relations)\n",
    "    \n",
    "    def get_all_entities(self):\n",
    "        return self.entities['entity'].unique()\n",
    "    \n",
    "    def get_all_relations(self):\n",
    "        return self.relations['relation'].unique()\n",
    "    \n",
    "    def get_all_entity_ids(self):\n",
    "        return self.entities['id'].unique()\n",
    "    \n",
    "    def get_all_relation_ids(self):\n",
    "        return self.relations['id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d25a2ad-1efe-4bc6-8c4e-9e843a287e03",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Relation Frequency in Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d923f37-ccd5-45b1-9d61-8e398e059ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_frequency_map = {}\n",
    "def get_relation_frequency_in_training_data(dataset):\n",
    "    if dataset in relation_frequency_map:\n",
    "        return relation_frequency_map[dataset]\n",
    "    \n",
    "    return _get_relation_frequency_in_training_data(dataset)\n",
    "\n",
    "@lazy_df_getter\n",
    "def _get_relation_frequency_in_training_data(dataset):    \n",
    "    path_to_dataset = os.path.join('experiments', '0_datasets', dataset)\n",
    "    path_to_training = os.path.join(path_to_dataset, 'train.txt')\n",
    "\n",
    "    training_df = pd.read_csv(path_to_training, sep='\\t', header=None)\n",
    "    training_df.columns = ['h', 'r', 't']\n",
    "\n",
    "    relations = training_df['r'].unique()\n",
    "    relation_counts = training_df['r'].value_counts()\n",
    "\n",
    "    df = pd.DataFrame(columns=['r_id','freq'])\n",
    "    eri = ERI(path_to_dataset)\n",
    "    for relation in relations:\n",
    "        df.loc[df.shape[0]] = [eri.get_relation_id(relation), relation_counts[relation]]\n",
    "    \n",
    "    df['norm_freq'] = (df['freq'] - df['freq'].min())/(df['freq'].max()-df['freq'].min())\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    relation_frequency_map[dataset] = df    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba5c41e-11c4-416f-855f-2c5f112e8753",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Entity Frequency in Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccecfb32-b899-4645-8bbc-9eb00b80d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_frequency_map = {}\n",
    "def get_entity_frequency_in_training_data(dataset):\n",
    "    if dataset in entity_frequency_map:\n",
    "        return entity_frequency_map[dataset]\n",
    "    \n",
    "    return _get_entity_frequency_in_training_data(dataset)\n",
    "\n",
    "@lazy_df_getter\n",
    "def _get_entity_frequency_in_training_data(dataset):\n",
    "    path_to_dataset = os.path.join('experiments', '0_datasets', dataset)\n",
    "    path_to_training = os.path.join(path_to_dataset, 'train.txt')\n",
    "\n",
    "    training_df = pd.read_csv(path_to_training, sep='\\t', header=None)\n",
    "    training_df.columns = ['h', 'r', 't']\n",
    "\n",
    "    h_entities = training_df['h'].unique()\n",
    "    t_entities = training_df['t'].unique()\n",
    "    entities = np.append(h_entities, t_entities)\n",
    "    entity_counts_h = training_df['h'].value_counts()\n",
    "    entity_counts_t = training_df['t'].value_counts()\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(columns=['h_id', 't_id', 'freq_h', 'freq_t'])\n",
    "    eri = ERI(path_to_dataset)\n",
    "    for entity in entities:   \n",
    "        e_id = eri.get_entity_id(entity)\n",
    "        hc = entity_counts_h[entity] if entity in h_entities else 0\n",
    "        tc = entity_counts_t[entity] if entity in t_entities else 0\n",
    "        df.loc[df.shape[0]] = [e_id, e_id, hc, tc]\n",
    "    \n",
    "    df['freq'] = df['freq_h'] + df['freq_t']\n",
    "    df['norm_freq'] = (df['freq'] - df['freq'].min())/(df['freq'].max()-df['freq'].min())\n",
    "    df['norm_freq_h'] = (df['freq_h'] - df['freq_h'].min())/(df['freq_h'].max()-df['freq_h'].min())\n",
    "    df['norm_freq_t'] = (df['freq_t'] - df['freq_t'].min())/(df['freq_t'].max()-df['freq_t'].min())\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    entity_frequency_map[dataset] = df    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d40a9-a562-4bef-a9fe-a50ea747e672",
   "metadata": {},
   "source": [
    "# Relation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d2bfe5b-9802-41d9-8557-932513f5f7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_relation_classes(dataset_name, threshold=0.85):\\n    path_to_dataset = os.path.join('experiments', '0_datasets', dataset_name)\\n    path_to_training = os.path.join(path_to_dataset, 'train.txt')\\n    path_to_valid = os.path.join(path_to_dataset, 'valid.txt')\\n    path_to_test = os.path.join(path_to_dataset, 'test.txt')\\n\\n    training_df = pd.read_csv(path_to_training, sep='\\t', header=None)\\n    valid_df = pd.read_csv(path_to_valid, sep='\\t', header=None)\\n    test_df = pd.read_csv(path_to_test, sep='\\t', header=None)\\n\\n    eri = ERI(path_to_dataset)\\n    relations = eri.get_all_relations()\\n\\n    data = pd.concat([training_df, valid_df, test_df])\\n    data.columns= ['h','r','t']\\n\\n    df=pd.DataFrame(columns=['r_id', 'relationClass'])\\n\\n    for relation in relations:\\n        data_for_h_r = data[data['r']==relation].groupby(['h', 'r']).agg(set)\\n        data_for_t_r = data[data['r']==relation].groupby(['t', 'r']).agg(set)\\n\\n        OneTo = 0\\n        Nto = 0\\n        for i in range(data_for_t_r.size):\\n            h_count  = len(data_for_t_r['h'].iloc[i])    \\n            if h_count > 1:\\n                Nto += 1\\n            elif h_count == 1:\\n                OneTo += 1 \\n\\n        toOne = 0\\n        toM = 0\\n        for i in range(data_for_h_r.size):\\n            t_count  = len(data_for_h_r['t'].iloc[i])\\n            if t_count > 1:\\n                toM += 1\\n            elif t_count == 1:\\n                toOne += 1          \\n\\n        xTo = ''\\n        if OneTo/(OneTo+Nto) > threshold:\\n            xTo = '1'\\n        else:\\n            xTo = 'N'\\n\\n        toX = ''\\n        if toOne/(toOne+toM) > threshold:\\n            toX = '1'\\n        else:\\n            toX = 'M'\\n\\n        df.loc[df.shape[0]] = [eri.get_relation_id(relation), f'{xTo}to{toX}']\\n    \\n    return df \\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def get_relation_classes(dataset_name, threshold=0.85):\n",
    "    path_to_dataset = os.path.join('experiments', '0_datasets', dataset_name)\n",
    "    path_to_training = os.path.join(path_to_dataset, 'train.txt')\n",
    "    path_to_valid = os.path.join(path_to_dataset, 'valid.txt')\n",
    "    path_to_test = os.path.join(path_to_dataset, 'test.txt')\n",
    "\n",
    "    training_df = pd.read_csv(path_to_training, sep='\\t', header=None)\n",
    "    valid_df = pd.read_csv(path_to_valid, sep='\\t', header=None)\n",
    "    test_df = pd.read_csv(path_to_test, sep='\\t', header=None)\n",
    "\n",
    "    eri = ERI(path_to_dataset)\n",
    "    relations = eri.get_all_relations()\n",
    "\n",
    "    data = pd.concat([training_df, valid_df, test_df])\n",
    "    data.columns= ['h','r','t']\n",
    "\n",
    "    df=pd.DataFrame(columns=['r_id', 'relationClass'])\n",
    "\n",
    "    for relation in relations:\n",
    "        data_for_h_r = data[data['r']==relation].groupby(['h', 'r']).agg(set)\n",
    "        data_for_t_r = data[data['r']==relation].groupby(['t', 'r']).agg(set)\n",
    "\n",
    "        OneTo = 0\n",
    "        Nto = 0\n",
    "        for i in range(data_for_t_r.size):\n",
    "            h_count  = len(data_for_t_r['h'].iloc[i])    \n",
    "            if h_count > 1:\n",
    "                Nto += 1\n",
    "            elif h_count == 1:\n",
    "                OneTo += 1 \n",
    "\n",
    "        toOne = 0\n",
    "        toM = 0\n",
    "        for i in range(data_for_h_r.size):\n",
    "            t_count  = len(data_for_h_r['t'].iloc[i])\n",
    "            if t_count > 1:\n",
    "                toM += 1\n",
    "            elif t_count == 1:\n",
    "                toOne += 1          \n",
    "\n",
    "        xTo = ''\n",
    "        if OneTo/(OneTo+Nto) > threshold:\n",
    "            xTo = '1'\n",
    "        else:\n",
    "            xTo = 'N'\n",
    "\n",
    "        toX = ''\n",
    "        if toOne/(toOne+toM) > threshold:\n",
    "            toX = '1'\n",
    "        else:\n",
    "            toX = 'M'\n",
    "\n",
    "        df.loc[df.shape[0]] = [eri.get_relation_id(relation), f'{xTo}to{toX}']\n",
    "    \n",
    "    return df \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a86d4aa-f9d5-4c7d-ad16-daee475911b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_classes_map = {}\n",
    "def get_relation_classes(checkpoint_path):\n",
    "    if checkpoint_path in relation_classes_map:\n",
    "        return relation_classes_map[checkpoint_path]\n",
    "    \n",
    "    return _get_relation_classes(checkpoint_path)\n",
    "\n",
    "@lazy_df_getter\n",
    "def _get_relation_classes(checkpoint_path):\n",
    "    checkpoint = load_checkpoint(checkpoint_path)\n",
    "    model = KgeModel.create_from(checkpoint)\n",
    "\n",
    "    eri = ERI(model.dataset.folder)\n",
    "\n",
    "    relation_strings = model.dataset.relation_strings()\n",
    "    relation_ids = [eri.get_relation_id(relation_string) for relation_string in relation_strings]\n",
    "    relation_types = index_relation_types(model.dataset)\n",
    "    \n",
    "    df = pd.DataFrame(data={'r_id': relation_ids, 'relationClass': relation_types})\n",
    "    \n",
    "    relation_classes_map[checkpoint_path] = df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb130472-803a-4f0c-b809-650e4eb9fecd",
   "metadata": {},
   "source": [
    "# Formatted Data Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3d005ba-fc3d-40ce-b46c-71db8ac52f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_data_name(dataset_name, symbolic_name, subsymbolic_name):\n",
    "    return f'{dataset_name}_{symbolic_name}_{subsymbolic_name}.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3718dbfc-e3c9-45b8-ab16-0efd73c1f9b6",
   "metadata": {},
   "source": [
    "# Calculate Mean Reciprocal Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6797f5a-24d5-48a2-a3f3-85080fb15fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_reciprocal_rank(ranks):\n",
    "    return (1/ranks.count())*((1/ranks).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b22a3c98-913b-48b4-832b-4f2fc94e9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hits_at_k(ranks, k):\n",
    "    return (ranks <= k).sum() / ranks.count()\n",
    "\n",
    "def calculate_hits_at_1(ranks):\n",
    "    return calculate_hits_at_k(ranks, 1)\n",
    "def calculate_hits_at_10(ranks):\n",
    "    return calculate_hits_at_k(ranks, 10)\n",
    "def calculate_hits_at_100(ranks):\n",
    "    return calculate_hits_at_k(ranks, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28678398-b3f7-4e9f-8b5e-f0bb932f1ee6",
   "metadata": {},
   "source": [
    "# Entity Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b846c230-a40d-40a2-b8a9-3b58e0089ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_matrix(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    added eps for numerical stability\n",
    "    \"\"\"\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.clamp(a_n, min=eps)\n",
    "    b_norm = b / torch.clamp(b_n, min=eps)\n",
    "    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "    return sim_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d073907-93bb-4dba-bff1-be1173ea218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_similarity_matrix_map = {}\n",
    "def get_entity_similarity_scores(checkpoint_path):\n",
    "    if checkpoint_path in entity_similarity_matrix_map:\n",
    "        return entity_similarity_matrix_map[checkpoint_path]\n",
    "    \n",
    "    return _get_entity_similarity_scores(checkpoint_path)\n",
    "\n",
    "@lazy_df_getter\n",
    "def _get_entity_similarity_scores(checkpoint_path):\n",
    "    checkpoint = load_checkpoint(checkpoint_path)\n",
    "    model = KgeModel.create_from(checkpoint)\n",
    "    \n",
    "    embeddings = model.get_s_embedder().embed_all()\n",
    "    entity_keys = model.dataset.entity_ids()\n",
    "    \n",
    "    cos_matrix = sim_matrix(embeddings, embeddings)\n",
    "    cos_scores = cos_matrix.sum(dim=1)\n",
    "    \n",
    "    eri = ERI(os.path.join('experiments', '0_datasets', model.dataset.folder.split('\\\\')[-1]))\n",
    "    entity_ids = eri.get_entity_ids(entity_keys)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'h_id': entity_ids,\n",
    "        't_id': entity_ids,\n",
    "        'entity_sim_score': cos_scores.detach().numpy()\n",
    "    })\n",
    "    \n",
    "    df['norm_entity_sim_score'] = (df['entity_sim_score'] - df['entity_sim_score'].min())/(df['entity_sim_score'].max()-df['entity_sim_score'].min())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5759ede4-8ec8-435e-ac5a-6ce5ada95c80",
   "metadata": {},
   "source": [
    "# Cluster Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16700244-786f-4ae6-96d5-e9b36e704140",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_clusters_map = {}\n",
    "def get_entity_clusters(checkpoint_path):\n",
    "    if checkpoint_path in entity_clusters_map:\n",
    "        return entity_clusters_map[checkpoint_path]\n",
    "    \n",
    "    return _get_entity_clusters(checkpoint_path)\n",
    "\n",
    "@lazy_df_getter\n",
    "def _get_entity_clusters(checkpoint_path, n_clusters=100):\n",
    "    checkpoint = load_checkpoint(checkpoint_path)\n",
    "    model = KgeModel.create_from(checkpoint)\n",
    "    \n",
    "    embeddings = model.get_s_embedder().embed_all().detach().numpy()\n",
    "    \n",
    "    entity_keys = model.dataset.entity_ids()\n",
    "    eri = ERI(os.path.join('experiments', '0_datasets', model.dataset.folder.split('\\\\')[-1]))\n",
    "    entity_ids = eri.get_entity_ids(entity_keys)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    df = pd.DataFrame({        \n",
    "        'h_id': entity_ids,\n",
    "        't_id': entity_ids,\n",
    "        'cluster_id': clusters\n",
    "    })\n",
    "    df = df.set_index('cluster_id').join(pd.DataFrame(df.cluster_id.value_counts()).rename(columns={'cluster_id': 'freq'}))\n",
    "    df['norm_freq'] = (df['freq'] - df['freq'].min())/(df['freq'].max()-df['freq'].min())\n",
    "    return df.reset_index().rename(columns=({'index': 'cluster_id'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5407ad3-8562-4387-a849-3a63d9641783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
