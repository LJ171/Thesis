
@misc{meilicke_reinforced_2020,
	title = {Reinforced {Anytime} {Bottom} {Up} {Rule} {Learning} for {Knowledge} {Graph} {Completion}},
	abstract = {Most of todays work on knowledge graph completion is concerned with sub-symbolic approaches that focus on the concept of embedding a given graph in a low dimensional vector space. Against this trend, we propose an approach called AnyBURL that is rooted in the symbolic space. Its core algorithm is based on sampling paths, which are generalized into Horn rules. Previously published results show that the prediction quality of AnyBURL is on the same level as current state of the art with the additional benefit of offering an explanation for the predicted fact. In this paper, we are concerned with two extensions of AnyBURL. Firstly, we change AnyBURLs interpretation of rules from \${\textbackslash}Theta\$-subsumption into \${\textbackslash}Theta\$-subsumption under Object Identity. Secondly, we introduce reinforcement learning to better guide the sampling process. We found out that reinforcement learning helps finding more valuable rules earlier in the search process. We measure the impact of both extensions and compare the resulting approach with current state of the art approaches. Our results show that AnyBURL outperforms most sub-symbolic methods.},
	publisher = {arXiv},
	author = {Meilicke, Christian and Chekol, Melisachew Wudage and Fink, Manuel and Stuckenschmidt, Heiner},
	month = apr,
	year = {2020},
	keywords = {\#todo},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\9CUA3UJU\\Meilicke et al. - 2020 - Reinforced Anytime Bottom Up Rule Learning for Kno.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\BP23X9VL\\2004.html:text/html},
}

@misc{meilicke_anyburl_2022,
	title = {{AnyBURL}},
	url = {https://web.informatik.uni-mannheim.de/AnyBURL/},
	urldate = {2022-01-28},
	author = {Meilicke, Christian},
	month = jan,
	year = {2022},
	keywords = {\#todo},
	file = {AnyBURL:C\:\\Users\\larsj\\Zotero\\storage\\APKG3PFJ\\AnyBURL.html:text/html},
}

@article{ott_safran_2021,
	title = {{SAFRAN}: {An} interpretable, rule-based link prediction method outperforming embedding models},
	abstract = {Neural embedding-based machine learning models have shown promise for predicting novel links in knowledge graphs. Unfortunately, their practical utility is diminished by their lack of interpretability. Recently, the fully interpretable, rule-based algorithm AnyBURL yielded highly competitive results on many general-purpose link prediction benchmarks. However, current approaches for aggregating predictions made by multiple rules are affected by redundancies. We improve upon AnyBURL by introducing the SAFRAN rule application framework, which uses a novel aggregation approach called Non-redundant Noisy-OR that detects and clusters redundant rules prior to aggregation. SAFRAN yields new state-of-the-art results for fully interpretable link prediction on the established general-purpose benchmarks FB15K-237, WN18RR and YAGO3-10. Furthermore, it exceeds the results of multiple established embedding-based algorithms on FB15K-237 and WN18RR and narrows the gap between rule-based and embedding-based algorithms on YAGO3-10.},
	urldate = {2022-01-28},
	journal = {arXiv preprint arXiv:2109.08002},
	author = {Ott, Simon and Meilicke, Christian and Samwald, Matthias},
	month = sep,
	year = {2021},
	keywords = {\#todo},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\VJGH5TY3\\Ott et al. - 2021 - SAFRAN An interpretable, rule-based link predicti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\86XBDNEQ\\2109.html:text/html},
}

@misc{noauthor_uma-pi1kge_2022,
	title = {uma-pi1/kge},
	copyright = {MIT},
	url = {https://github.com/uma-pi1/kge},
	abstract = {LibKGE - A knowledge graph embedding library for reproducible research},
	urldate = {2022-01-28},
	publisher = {Chair of Data Analytics, University of Mannheim, Germany},
	month = jan,
	year = {2022},
	note = {original-date: 2019-02-22T18:39:36Z},
}

@inproceedings{meilicke_anytime_2019,
	title = {Anytime {Bottom}-{Up} {Rule} {Learning} for {Knowledge} {Graph} {Completion}},
	abstract = {We propose an anytime bottom-up technique for learning logical rules from large knowledge graphs. We apply the learned rules to predict candidates in the context of knowledge graph completion. Our approach outperforms other rule-based approaches and it is competitive with current state of the art, which is based on latent representations. Besides, our approach is significantly faster, requires less computational resources, and yields an explanation in terms of the rules that propose a candidate.},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Meilicke, Christian and Chekol, Melisachew Wudage and Ruffinelli, Daniel and Stuckenschmidt, Heiner},
	month = aug,
	year = {2019},
	keywords = {\#todo},
	pages = {3137--3143},
	file = {Meilicke et al. - 2019 - Anytime Bottom-Up Rule Learning for Knowledge Grap.pdf:C\:\\Users\\larsj\\Zotero\\storage\\XXW898RH\\Meilicke et al. - 2019 - Anytime Bottom-Up Rule Learning for Knowledge Grap.pdf:application/pdf},
}

@inproceedings{meilicke_why_2021,
	title = {Why a {Naive} {Way} to {Combine} {Symbolic} and {Latent} {Knowledge} {Base} {Completion} {Works} {Surprisingly} {Well}},
	abstract = {We compare a rule-based approach for knowledge graph completion against current state-of-the-art, which is based on embeddings. Instead of focusing on aggregated metrics, we look at several examples that illustrate essential differences between symbolic and latent approaches. Based on our insights, we construct a simple method to combine the outcome of rule-based and latent approaches in a post-processing step. Our method improves the results constantly for each model and dataset used in our experiments.},
	language = {en},
	booktitle = {Proceedings of the 3rd {Conference} on {Automated} {Knowledge} {Base} {Construction}},
	author = {Meilicke, Christian and Betz, Patrick and Stuckenschmidt, Heiner},
	year = {2021},
	keywords = {\#todo},
	file = {Meilicke et al. - Why a Naive Way to Combine Symbolic and Latent Kno.pdf:C\:\\Users\\larsj\\Zotero\\storage\\98EZQLT8\\Meilicke et al. - Why a Naive Way to Combine Symbolic and Latent Kno.pdf:application/pdf},
}

@article{socher_reasoning_2013,
	title = {Reasoning {With} {Neural} {Tensor} {Networks} for {Knowledge} {Base} {Completion}},
	volume = {26},
	urldate = {2022-01-28},
	journal = {Advances in neural information processing systems},
	author = {Socher, Richard and Chen, Danqi and Manning, Christopher D and Ng, Andrew},
	year = {2013},
	file = {Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\TGCF4WEF\\Socher et al. - 2013 - Reasoning With Neural Tensor Networks for Knowledg.pdf:application/pdf},
}

@inproceedings{bordes_learning_2011,
	title = {Learning {Structured} {Embeddings} of {Knowledge} {Bases}},
	abstract = {Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigorous symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like nat- ural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning meth- ods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text.},
	booktitle = {Proceedings of the {Twenty}-{Fifth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Bordes, Antoine and Weston, Jason and Collobert, Ronan and Bengio, Yoshua},
	month = aug,
	year = {2011},
	file = {Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\8JNI9V6R\\Bordes et al. - 2011 - Learning Structured Embeddings of Knowledge Bases.pdf:application/pdf;Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\UW7QSGHS\\3659.html:text/html},
}

@article{bordes_translating_2013,
	title = {Translating {Embeddings} for {Modeling} {Multi}-relational {Data}},
	volume = {26},
	journal = {Advances in Neural Information Processing Systems},
	author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
	year = {2013},
	file = {Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\EBUB82X5\\Bordes et al. - 2013 - Translating Embeddings for Modeling Multi-relation.pdf:application/pdf},
}

@inproceedings{ji_knowledge_2015,
	title = {Knowledge {Graph} {Embedding} via {Dynamic} {Mapping} {Matrix}},
	volume = {1},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ji, Guoliang and He, Shizhu and Xu, Liheng and Liu, Kang and Zhao, Jun},
	month = jul,
	year = {2015},
	pages = {687--696},
	file = {Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\SVU8WUKN\\Ji et al. - 2015 - Knowledge Graph Embedding via Dynamic Mapping Matr.pdf:application/pdf},
}

@book{fensel_introduction_2020,
	title = {Introduction: {What} {Is} a {Knowledge} {Graph}?},
	volume = {1},
	isbn = {978-3-030-37439-6},
	abstract = {Since its inception by Google, Knowledge Graph has become a term that is recently ubiquitously used yet does not have a well-established definition. This section attempts to derive a definition for Knowledge Graphs by compiling existing definitions made in the literature and considering the distinctive characteristics of previous efforts for tackling the data integration challenge we are facing today. Our attempt to make a conceptual definition is complemented with an empirical survey of existing Knowledge Graphs. This section lays the foundation for the remainder of the book, as it provides a common understanding on certain concepts and motivation to build Knowledge Graphs in the first place.},
	publisher = {Springer International Publishing},
	author = {Fensel, Dieter and {\c S}im{\c s}ek, Umutcan and Angele, Kevin and Huaman, Elwin and K{\"a}rle, Elias and Panasiuk, Oleksandra and Toma, Ioan and Umbrich, J{\"u}rgen and Wahler, Alexander},
	editor = {Fensel, Dieter and {\c S}im{\c s}ek, Umutcan and Angele, Kevin and Huaman, Elwin and K{\"a}rle, Elias and Panasiuk, Oleksandra and Toma, Ioan and Umbrich, J{\"u}rgen and Wahler, Alexander},
	year = {2020},
}

@book{milton_knowledge_2007,
	title = {Knowledge {Acquisition} in {Practice}: {A} {Step}-by-step {Guide}},
	isbn = {978-1-84628-861-6},
	abstract = {Recent years have seen an upsurge of interest in knowledge. Leading organisations now recognise the importance of identifying what they know, sharing what they know and using what they know for maximum benefit. Many organisations employ knowledge engineers to capture knowledge from experts using the principles and techniques of knowledge engineering. The emphasis is on a structured approach built on a sound understanding of the psychology of expertise and making use of knowledge modelling methods and the latest web technologies.   Knowledge Acquisition in Practice is the first book to provide a detailed step-by-step guide to the methods and practical aspects of acquiring, modelling, storing and sharing knowledge. The reader is led through 47 steps from the inception of a project to its successful conclusion. Each step is described in terms of the reasons for the step, the required resources, the activities to be undertaken, and the solutions to common problems. In addition, each step has a checklist which lists the key items that should be achieved during the step.  Knowledge Acquisition in Practice will be of value to knowledge engineers, knowledge workers, knowledge officers and ontological engineers. The book will also be of interest to students and researchers of AI, computer science and business studies.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Milton, Nicholas Ross},
	month = may,
	year = {2007},
	keywords = {Business \& Economics / Business Mathematics, Business \& Economics / Information Management, Computers / Artificial Intelligence / General, Computers / Information Technology, Language Arts \& Disciplines / Library \& Information Science / General, Technology \& Engineering / Engineering (General), Technology \& Engineering / General},
}

@article{paulheim_automatic_2017,
	title = {Automatic {Knowledge} {Graph} {Refinement}: {A} {Survey} of {Approaches} and {Evaluation} {Methods}},
	volume = {8},
	abstract = {In the recent years, different web knowledge graphs, both free and commercial, have been created, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and NLP methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.},
	number = {3},
	journal = {Semantic web},
	author = {Paulheim, Heiko},
	year = {2017},
	pages = {489--508},
	file = {Paulheim - Automatic Knowledge Graph Refinement A Survey of A.pdf:C\:\\Users\\larsj\\Zotero\\storage\\BYHNW9JP\\Paulheim - Automatic Knowledge Graph Refinement A Survey of A.pdf:application/pdf},
}

@article{paulheim_knowledge_2016,
	title = {Knowledge graph refinement: {A} survey of approaches and evaluation methods},
	volume = {8},
	issn = {22104968, 15700844},
	shorttitle = {Knowledge graph refinement},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-160218},
	doi = {10.3233/SW-160218},
	abstract = {In the recent years, different Web knowledge graphs, both free and commercial, have been created. While Google coined the term {\textquotedblleft}Knowledge Graph{\textquotedblright} in 2012, there are also a few openly available knowledge graphs, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and linguistic methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of such knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.},
	language = {en},
	number = {3},
	urldate = {2022-05-11},
	journal = {Semantic Web},
	author = {Paulheim, Heiko},
	editor = {Cimiano, Philipp},
	month = dec,
	year = {2016},
	pages = {489--508},
	file = {Paulheim - 2016 - Knowledge graph refinement A survey of approaches.pdf:C\:\\Users\\larsj\\Zotero\\storage\\4HCUHBWU\\Paulheim - 2016 - Knowledge graph refinement A survey of approaches.pdf:application/pdf},
}

@book{talburt_entity_2011,
	title = {Entity {Resolution} and {Information} {Quality}},
	isbn = {978-0-12-381973-4},
	abstract = {Entity Resolution and Information Quality presents topics and definitions, and clarifies confusing terminologies regarding entity resolution and information quality. It takes a very wide view of IQ, including its six-domain framework and the skills formed by the International Association for Information and Data Quality \{\vphantom{\}}IAIDQ). The book includes chapters that cover the principles of entity resolution and the principles of Information Quality, in addition to their concepts and terminology. It also discusses the Fellegi-Sunter theory of record linkage, the Stanford Entity Resolution Framework, and the Algebraic Model for Entity Resolution, which are the major theoretical models that support Entity Resolution. In relation to this, the book briefly discusses entity-based data integration (EBDI) and its model, which serve as an extension of the Algebraic Model for Entity Resolution. There is also an explanation of how the three commercial ER systems operate and a description of the non-commercial open-source system known as OYSTER. The book concludes by discussing trends in entity resolution research and practice. Students taking IT courses and IT professionals will find this book invaluable.First authoritative reference explaining entity resolution and how to use it effectivelyProvides practical system design advice to help you get a competitive advantage Includes a companion site with synthetic customer data for applicatory exercises, and access to a Java-based Entity Resolution program.},
	publisher = {Elsevier},
	author = {Talburt, John},
	month = jan,
	year = {2011},
	keywords = {Business \& Economics / Sales \& Selling / General, Computers / Business \& Productivity Software / Business Intelligence, Computers / Business \& Productivity Software / General, Computers / Database Administration \& Management, Computers / Management Information Systems},
}

@inproceedings{baumgartner_entity_2021,
	title = {Entity {Prediction} in {Knowledge} {Graphs} with {Joint} {Embeddings}},
	abstract = {Knowledge Graphs (KGs) have become increasingly popular in the recent years. However, as knowledge constantly grows and changes, it is inevitable to extend existing KGs with entities that emerged or became relevant to the scope of the KG after its creation. Research on updating KGs typically relies on extracting named entities and relations from text. However, these approaches cannot infer entities or relations that were not explicitly stated. Alternatively, embedding models exploit implicit structural regularities to predict missing relations, but cannot predict missing entities. In this article, we introduce a novel method to enrich a KG with new entities given their textual description. Our method leverages joint embedding models, hence does not require entities or relations to be named explicitly. We show that our approach can identify new concepts in a document corpus and transfer them into the KG, and we find that the performance of our method improves substantially when extended with techniques from association rule mining, text mining, and active learning.},
	booktitle = {Proceedings of the {Fifteenth} {Workshop} on {Graph}-{Based} {Methods} for {Natural} {Language} {Processing} ({TextGraphs}-15)},
	publisher = {Association for Computational Linguistics},
	author = {Baumgartner, Matthias and Dell'Aglio, Daniele and Bernstein, Abraham},
	month = jun,
	year = {2021},
	pages = {22--31},
	file = {Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\INJIVYJ8\\Baumgartner et al. - 2021 - Entity Prediction in Knowledge Graphs with Joint E.pdf:application/pdf},
}

@book{golbeck_analyzing_2013,
	title = {Analyzing the {Social} {Web}},
	volume = {1},
	isbn = {978-0-12-405856-9},
	abstract = {Analyzing the Social Web provides a framework for the analysis of public data currently available and being generated by social networks and social media, like Facebook, Twitter, and Foursquare. Access and analysis of this public data about people and their connections to one another allows for new applications of traditional social network analysis techniques that let us identify things like who are the most important or influential people in a network, how things will spread through the network, and the nature of peoples' relationships. Analyzing the Social Web introduces you to these techniques, shows you their application to many different types of social media, and discusses how social media can be used as a tool for interacting with the online public. Presents interactive social applications on the web, and the types of analysis that are currently conducted in the study of social media Covers the basics of network structures for beginners, including measuring methods for describing nodes, edges, and parts of the network Discusses the major categories of social media applications or phenomena and shows how the techniques presented can be applied to analyze and understand the underlying data Provides an introduction to information visualization, particularly network visualization techniques, and methods for using them to identify interesting features in a network, generate hypotheses for analysis, and recognize patterns of behavior Includes a supporting website with lecture slides, exercises, and downloadable social network data sets that can be used can be used to apply the techniques presented in the book},
	publisher = {Newnes},
	author = {Golbeck, Jennifer},
	month = feb,
	year = {2013},
	keywords = {Computers / Human-Computer Interaction (HCI), Computers / Internet / General},
}

@inproceedings{ilkou_symbolic_2020,
	title = {Symbolic {Vs} {Sub}-symbolic {AI} {Methods}: {Friends} or {Enemies}?},
	abstract = {There is a long and unresolved debate between the symbolic and sub-symbolic methods. However, in recent years, there is a push towards in-between methods. In this work, we provide a comprehensive overview of the symbolic, sub-symbolic and in-between approaches focused in the domain of knowledge graphs, namely, schema representation, schema matching, knowledge graph completion, link prediction, entity resolution, entity classification and triple classification. We critically present key characteristics, advantages and disadvantages of the main algorithms in each domain, and review the use of these methods in knowledge graph related applications.},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	author = {Ilkou, Eleni and Koutraki, Maria},
	month = nov,
	year = {2020},
	file = {paper06.pdf:C\:\\Users\\larsj\\Downloads\\paper06.pdf:application/pdf},
}

@article{hatzilygeroudis_neuro-symbolic_2005,
	title = {Neuro-{Symbolic} {Approaches} for {Knowledge} {Representation} in {Expert} {Systems}},
	volume = {1},
	abstract = {In this paper, we first present and compare existing categorization schemes for neuro-symbolic approaches. We then stress the point that not all hybrid neuro-symbolic approaches can be accommodated by existing categories. Such a case is rule-based neuro-symbolic approaches that propose a unified knowledge representation scheme suitable for use in expert systems. That kind of integrated schemes have the two component approaches tightly and indistinguishably integrated, offer an interactive inference engine and can provide explanations. Therefore, we introduce a new category of neuro-symbolic integrations, namely {\textquoteleft}representational integrations{\textquoteright}. Furthermore, two sub-categories of representational integrations are distinguished, based on which of the two component approaches of the integrations is given pre-eminence. Representative approaches as well as advantages and disadvantages of both sub-categories are discussed.},
	number = {3-4},
	journal = {International Journal of Hybrid Intelligent Systems},
	author = {Hatzilygeroudis, Ioannis and Prentzas, Jim},
	month = jan,
	year = {2005},
	pages = {111--126},
	file = {Hatzilygeroudis and Prentzas - 2005 - Neuro-Symbolic Approaches for Knowledge Representa.pdf:C\:\\Users\\larsj\\Zotero\\storage\\9B93VMFQ\\Hatzilygeroudis and Prentzas - 2005 - Neuro-Symbolic Approaches for Knowledge Representa.pdf:application/pdf},
}

@article{nickel_review_2015,
	title = {A {Review} of {Relational} {Machine} {Learning} for {Knowledge} {Graphs}},
	volume = {104},
	abstract = {Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be "trained" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy},
	year = {2015},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {11--33},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\2GX972B6\\Nickel et al. - 2016 - A Review of Relational Machine Learning for Knowle.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\65NHX499\\1503.html:text/html},
}

@article{zou_survey_2020,
	title = {A {Survey} on {Application} of {Knowledge} {Graph}},
	volume = {1487},
	abstract = {Knowledge graphs, representation of information as a semantic graph, have caused wide concern in both industrial and academic world. Their property of providing semantically structured information has brought important possible solutions for many tasks including question answering, recommendation and information retrieval, and is considered to offer great promise for building more intelligent machines by many researchers. Although knowledge graphs have already supported multiple {\textquotedblleft}Big Data{\textquotedblright} applications in all sorts of commercial and scientific domains since Google coined this term in 2012, there was no previous study give a systemically review of the application of knowledge graphs. Therefore, unlike other related work which focuses on the construction techniques of knowledge graphs, this present paper aims at providing a first survey on these applications stemming from different domains. This paper also points out that while important advancements of applying knowledge graphs' great ability of providing semantically structured information into specific domains have been made in recent years, several aspects still remain to be explored.},
	number = {1},
	journal = {Journal of Physics: Conference Series},
	author = {Zou, Xiaohan},
	month = mar,
	year = {2020},
	pages = {012016},
	file = {Zou - 2020 - A Survey on Application of Knowledge Graph.pdf:C\:\\Users\\larsj\\Zotero\\storage\\D5CTM4W7\\Zou - 2020 - A Survey on Application of Knowledge Graph.pdf:application/pdf},
}

@misc{singhal_introducing_2012,
	title = {Introducing the {Knowledge} {Graph}: things, not strings},
	shorttitle = {Introducing the {Knowledge} {Graph}},
	url = {https://blog.google/products/search/introducing-knowledge-graph-things-not/},
	abstract = {We hope this will give you a more complete picture of your interest, provide smarter search results, and pique your curiosity.},
	urldate = {2022-05-17},
	journal = {Google},
	author = {Singhal, Amit},
	month = may,
	year = {2012},
	file = {Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\F3CF9BWH\\introducing-knowledge-graph-things-not.html:text/html},
}

@article{bianchi_knowledge_2020,
	title = {Knowledge {Graph} {Embeddings} and {Explainable} {AI}},
	volume = {47},
	abstract = {Knowledge graph embeddings are now a widely adopted approach to knowledge representation in which entities and relationships are embedded in vector spaces. In this chapter, we introduce the reader to the concept of knowledge graph embeddings by explaining what they are, how they can be generated and how they can be evaluated. We summarize the state-of-the-art in this field by describing the approaches that have been introduced to represent knowledge in the vector space. In relation to knowledge representation, we consider the problem of explainability, and discuss models and methods for explaining predictions obtained via knowledge graph embeddings.},
	journal = {Knowledge Graphs for Explainable Artificial Intelligence: Foundations, Applications and Challenges},
	author = {Bianchi, Federico and Rossiello, Gaetano and Costabello, Luca and Palmonari, Matteo and Minervini, Pasquale},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {49},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\QD83KNMR\\Bianchi et al. - 2020 - Knowledge Graph Embeddings and Explainable AI.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\EXTYT2GP\\2004.html:text/html},
}

@inproceedings{trouillon_complex_2016,
	title = {Complex {Embeddings} for {Simple} {Link} {Prediction}},
	booktitle = {Proceedings of the {International} conference on machine learning},
	author = {Trouillon, Theo and Welbl, Johannes and Riedel, Sebastian},
	year = {2016},
	pages = {2071--2080},
	file = {Trouillon et al. - Complex Embeddings for Simple Link Prediction.pdf:C\:\\Users\\larsj\\Zotero\\storage\\CMD2JDD7\\Trouillon et al. - Complex Embeddings for Simple Link Prediction.pdf:application/pdf},
}

@inproceedings{nickel_three-way_2011,
	title = {A {Three}-{Way} {Model} for {Collective} {Learning} on {Multi}-{Relational} {Data}},
	abstract = {Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.},
	booktitle = {Proceedings of the {The} 28th {International} {Conference} on {Machine} {Learning}},
	author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
	year = {2011},
	file = {Nickel et al. - A Three-Way Model for Collective Learning on Multi.pdf:C\:\\Users\\larsj\\Zotero\\storage\\7RAJQLMF\\Nickel et al. - A Three-Way Model for Collective Learning on Multi.pdf:application/pdf},
}

@inproceedings{krompas_non-negative_2013,
	title = {Non-{Negative} {Tensor} {Factorization} with rescal},
	abstract = {Non-negative data is generated by a broad selection of applications today, e.g in gene expression analysis or imaging. Many factorization techniques have been extended to account for this natural constraint and have become very popular due to their decomposition into interpretable latent factors. Generally relational data like protein interaction networks or social network data can also be seen as naturally non-negative. In this work, we extend the RESCAL tensor factorization, which has shown state-of-the-art results for multi-relational learning, to account for non-negativity by employing multiplicative update rules. We study the performance via these approaches on various benchmark datasets and show that a non-negativity constraint can be introduced by losing only little in terms of predictive quality in most of the cases but simultaneously increasing the sparsity of the factors significantly compared to the original RESCAL algorithm.},
	booktitle = {Proceedings of the {Tensor} {Methods} for {Machine} {Learning}, {ECML} workshop},
	author = {Krompa{\ss}, Denis and Nickel, Maximilian and Jiang, Xueyan and Tresp, Volker},
	year = {2013},
	pages = {1--10},
	file = {Krompa{\ss} et al. - Non-Negative Tensor Factorization with.pdf:C\:\\Users\\larsj\\Zotero\\storage\\XVRBH8QE\\Krompa{\ss} et al. - Non-Negative Tensor Factorization with.pdf:application/pdf},
}

@misc{safavi_codex_2020,
	title = {{CoDEx}: {A} {Comprehensive} {Knowledge} {Graph} {Completion} {Benchmark}},
	abstract = {We present CODEX, a set of knowledge graph COmpletion Datasets EXtracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CODEX comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. To characterize CODEX, we contribute thorough empirical analyses and benchmarking experiments. First, we analyze each CODEX dataset in terms of logical relation patterns. Next, we report baseline link prediction and triple classification results on CODEX for five extensively tuned embedding models. Finally, we differentiate CODEX from the popular FB15K-237 knowledge graph completion dataset by showing that CODEX covers more diverse and interpretable content, and is a more difficult link prediction benchmark. Data, code, and pretrained models are available at https://bit.ly/2EPbrJs.},
	publisher = {arXiv},
	author = {Safavi, Tara and Koutra, Danai},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {Safavi and Koutra - 2020 - CoDEx A Comprehensive Knowledge Graph Completion .pdf:C\:\\Users\\larsj\\Zotero\\storage\\JHJF6KTY\\Safavi and Koutra - 2020 - CoDEx A Comprehensive Knowledge Graph Completion .pdf:application/pdf},
}

@article{chen_knowledge_2020,
	title = {Knowledge {Graph} {Completion}: {A} {Review}},
	volume = {8},
	abstract = {Knowledge graph completion (KGC) is a hot topic in knowledge graph construction and related applications, which aims to complete the structure of knowledge graph by predicting the missing entities or relationships in knowledge graph and mining unknown facts. Starting from the definition and types of KGC, existing technologies for KGC are analyzed in categories. From the evolving point of view, the KGC technologies could be divided into traditional and representation learning based methods. The former mainly includes rule-based reasoning method, probability graph model, such as Markov logic network, and graph computation based method. The latter further includes translation model based, semantic matching model based, representation learning based and other neural network model based methods. In this article, different KGC technologies are introduced, including their advantages, disadvantages and applicable fields. Finally the main challenges and problems faced by the KGC are discussed, as well as the potential research directions.},
	journal = {IEEE Access},
	author = {Chen, Zhe and Wang, Yuehan and Zhao, Bin and Cheng, Jing and Zhao, Xin and Duan, Zongtao},
	year = {2020},
	keywords = {Cognition, Computational modeling, deep learning, entity prediction, Knowledge graph, knowledge graph completion, Knowledge representation, Natural language processing, relation prediction, Semantics, Task analysis},
	pages = {192435--192456},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\larsj\\Zotero\\storage\\VVT8ADK9\\9220143.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\LNGTIVYG\\Chen et al. - 2020 - Knowledge Graph Completion A Review.pdf:application/pdf},
}

@inproceedings{nickel_factorizing_2012,
	title = {Factorizing {YAGO}: scalable machine learning for linked data},
	isbn = {978-1-4503-1229-5},
	abstract = {Vast amounts of structured information have been published in the Semantic Web{\textquoteright}s Linked Open Data (LOD) cloud and their size is still growing rapidly. Yet, access to this information via reasoning and querying is sometimes difficult, due to LOD{\textquoteright}s size, partial data inconsistencies and inherent noisiness. Machine Learning offers an alternative approach to exploiting LOD{\textquoteright}s data with the advantages that Machine Learning algorithms are typically robust to both noise and data inconsistencies and are able to efficiently utilize nondeterministic dependencies in the data. From a Machine Learning point of view, LOD is challenging due to its relational nature and its scale. Here, we present an efficient approach to relational learning on LOD data, based on the factorization of a sparse tensor that scales to data consisting of millions of entities, hundreds of relations and billions of known facts. Furthermore, we show how ontological knowledge can be incorporated in the factorization to improve learning results and how computation can be distributed across multiple nodes. We demonstrate that our approach is able to factorize the YAGO 2 core ontology and globally predict statements for this large knowledge base using a single dual-core desktop computer. Furthermore, we show experimentally that our approach achieves good results in several relational learning tasks that are relevant to Linked Data. Once a factorization has been computed, our model is able to predict efficiently, and without any additional training, the likelihood of any of the 4.3 {\textperiodcentered} 1014 possible triples in the YAGO 2 core ontology.},
	booktitle = {Proceedings of the 21st international conference on {World} {Wide} {Web}},
	author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
	month = apr,
	year = {2012},
	pages = {271--280},
	file = {Nickel et al. - 2012 - Factorizing YAGO scalable machine learning for li.pdf:C\:\\Users\\larsj\\Zotero\\storage\\SCMQ4ZT7\\Nickel et al. - 2012 - Factorizing YAGO scalable machine learning for li.pdf:application/pdf},
}

@inproceedings{yang_embedding_2015,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) ={\textgreater} Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Learning} {Representations}},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	month = aug,
	year = {2015},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\MJRE6UWX\\Yang et al. - 2015 - Embedding Entities and Relations for Learning and .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\LYIV9IJR\\1412.html:text/html},
}

@article{hogan_knowledge_2021,
	title = {Knowledge {Graphs}},
	volume = {12},
	abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.},
	number = {2},
	journal = {Synthesis Lectures on Data, Semantics, and Knowledge},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d'Amato, Claudia and de Melo, Gerard and Gutierrez, Claudio and Gayo, Jos{\'e} Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
	month = sep,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning},
	pages = {1--257},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\VJHRHGTL\\Hogan et al. - 2022 - Knowledge Graphs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\AUU3IMU6\\2003.html:text/html},
}

@inproceedings{dettmers_convolutional_2018,
	title = {Convolutional {2D} {Knowledge} {Graph} {Embeddings}},
	volume = {32},
	abstract = {Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models -- which potentially limits performance. In this work, we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree -- which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set -- however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets -- deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across most datasets.},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	publisher = {AAAI},
	author = {Dettmers, Tim and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
	month = jul,
	year = {2018},
	keywords = {Computer Science - Machine Learning},
	pages = {1811--1818},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\UBEUY8W7\\Dettmers et al. - 2018 - Convolutional 2D Knowledge Graph Embeddings.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\UJSBZF5C\\1707.html:text/html},
}

@article{zhang_parallel_1990,
	title = {Parallel distributed processing model with local space-invariant interconnections and its optical architecture},
	volume = {29},
	abstract = {This paper proposes a parallel distributed processing model with local space-invariant interconnections, which is more readily implemented by optics and is able to classify patterns correctly, even if they have been shifted or distorted. Error backpropagation is used as a training algorithm. Computer simulation results presented indicate that the processing is effective and the network can deal with the shifted or distorted patterns. Moreover, the optical implementation architecture using matched filters for the model is discussed.},
	number = {32},
	journal = {Applied Optics},
	author = {Zhang, Wei and Itoh, Kazuyoshi and Tanida, Jun and Ichioka, Yoshiki},
	month = nov,
	year = {1990},
	pages = {4790--4797},
	file = {Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\4J8N58YQ\\viewmedia.html:text/html},
}

@article{chandra_horn_1985,
	title = {Horn clause queries and generalizations},
	volume = {2},
	number = {1},
	journal = {The Journal of Logic Programming},
	author = {Chandra, Ashok K. and Harel, David},
	month = apr,
	year = {1985},
	pages = {1--15},
	file = {Chandra and Harel - 1985 - Horn clause queries and generalizations.pdf:C\:\\Users\\larsj\\Zotero\\storage\\WRLGBPLR\\Chandra and Harel - 1985 - Horn clause queries and generalizations.pdf:application/pdf},
}

@article{wang_knowledge_2017,
	title = {Knowledge {Graph} {Embedding}: {A} {Survey} of {Approaches} and {Applications}},
	volume = {29},
	abstract = {Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.},
	number = {12},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Wang, Quan and Mao, Zhendong and Wang, Bin and Guo, Li},
	month = dec,
	year = {2017},
	keywords = {Graphical models, Knowledge discovery, knowledge graph embedding, latent factor models, Market research, Matrix decomposition, Semantics, Statistical analysis, Statistical relational learning, Systematics, tensor/matrix factorization models},
	pages = {2724--2743},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\larsj\\Zotero\\storage\\ZHNDEZLT\\8047276].html:text/html},
}

@inproceedings{galarraga_amie_2013,
	title = {{AMIE}: association rule mining under incomplete evidence in ontological knowledge bases},
	isbn = {978-1-4503-2035-1},
	shorttitle = {{AMIE}},
	abstract = {Recent advances in information extraction have led to huge knowledge bases (KBs), which capture knowledge in a machine-readable format. Inductive Logic Programming (ILP) can be used to mine logical rules from the KB. These rules can help deduce and add missing knowledge to the KB. While ILP is a mature field, mining logical rules from KBs is different in two aspects: First, current rule mining systems are easily overwhelmed by the amount of data (state-of-the art systems cannot even run on today{\textquoteright}s KBs). Second, ILP usually requires counterexamples. KBs, however, implement the open world assumption (OWA), meaning that absent data cannot be used as counterexamples. In this paper, we develop a rule mining model that is explicitly tailored to support the OWA scenario. It is inspired by association rule mining and introduces a novel measure for confidence. Our extensive experiments show that our approach outperforms state-of-the-art approaches in terms of precision and coverage. Furthermore, our system, AMIE, mines rules orders of magnitude faster than state-of-the-art approaches.},
	booktitle = {Proceedings of the 22nd international conference on {World} {Wide} {Web}},
	publisher = {ACM Press},
	author = {Gal{\'a}rraga, Luis Antonio and Teflioudi, Christina and Hose, Katja and Suchanek, Fabian},
	year = {2013},
	pages = {413--422},
	file = {Gal{\'a}rraga et al. - 2013 - AMIE association rule mining under incomplete evi.pdf:C\:\\Users\\larsj\\Zotero\\storage\\4JUIDAPN\\Gal{\'a}rraga et al. - 2013 - AMIE association rule mining under incomplete evi.pdf:application/pdf},
}

@inproceedings{ghiasnezhad_omran_scalable_2018,
	address = {Stockholm, Sweden},
	title = {Scalable {Rule} {Learning} via {Learning} {Representation}},
	isbn = {978-0-9992411-2-7},
	abstract = {We study the problem of learning first-order rules from large Knowledge Graphs (KGs). With recent advancement in information extraction, vast data repositories in the KG format have been obtained such as Freebase and YAGO. However, traditional techniques for rule learning are not scalable for KGs. This paper presents a new approach RLvLR to learning rules from KGs by using the technique of embedding in representation learning together with a new sampling method. Experimental results show that our system outperforms some state-ofthe-art systems. Specifically, for massive KGs with hundreds of predicates and over 10M facts, RLvLR is much faster and can learn much more quality rules than major systems for rule learning in KGs such as AMIE+. We also used the RLvLR-mined rules in an inference module to carry out the link prediction task. In this task, RLvLR outperformed Neural LP, a state-of-the-art link prediction system, in both runtime and accuracy.},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Ghiasnezhad Omran, Pouya and Wang, Kewen and Wang, Zhe},
	month = jul,
	year = {2018},
	pages = {2149--2155},
	file = {Ghiasnezhad Omran et al. - 2018 - Scalable Rule Learning via Learning Representation.pdf:C\:\\Users\\larsj\\Zotero\\storage\\RL4Z8FC3\\Ghiasnezhad Omran et al. - 2018 - Scalable Rule Learning via Learning Representation.pdf:application/pdf},
}

@inproceedings{toutanova_observed_2015,
	title = {Observed versus latent features for knowledge base and text inference},
	booktitle = {Proceedings of the 3rd {Workshop} on {Continuous} {Vector} {Space} {Models} and their {Compositionality}},
	author = {Toutanova, Kristina and Chen, Danqi},
	month = jul,
	year = {2015},
	pages = {57--66},
	file = {Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\7US52AYQ\\Toutanova and Chen - 2015 - Observed versus latent features for knowledge base.pdf:application/pdf},
}

@inproceedings{broscheit_libkge_2020,
	title = {{LibKGE} - {A} knowledge graph embedding library for reproducible research},
	abstract = {LIBKGE1 is an open-source PyTorch-based library for training, hyperparameter optimization, and evaluation of knowledge graph embedding models for link prediction. The key goals of LIBKGE are to enable reproducible research, to provide a framework for comprehensive experimental studies, and to facilitate analyzing the contributions of individual components of training methods, model architectures, and evaluation methods. LIBKGE is highly configurable and every experiment can be fully reproduced with a single configuration file. Individual components are decoupled to the extent possible so that they can be mixed and matched with each other. Implementations in LIBKGE aim to be as efficient as possible without leaving the scope of Python/Numpy/PyTorch. A comprehensive logging mechanism and tooling facilitates indepth analysis. LIBKGE provides implementations of common knowledge graph embedding models and training methods, and new ones can be easily added. A comparative study (Ruffinelli et al., 2020) showed that LIBKGE reaches competitive to state-of-the-art performance for many models with a modest amount of automatic hyperparameter tuning.},
	language = {en},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Broscheit, Samuel and Ruffinelli, Daniel and Kochsiek, Adrian and Betz, Patrick and Gemulla, Rainer},
	year = {2020},
	pages = {165--174},
	file = {Broscheit et al. - 2020 - LibKGE - A knowledge graph embedding library for r.pdf:C\:\\Users\\larsj\\Zotero\\storage\\IVFETEPL\\Broscheit et al. - 2020 - LibKGE - A knowledge graph embedding library for r.pdf:application/pdf},
}

@misc{shang_end--end_2018,
	title = {End-to-end {Structure}-{Aware} {Convolutional} {Networks} for {Knowledge} {Base} {Completion}},
	abstract = {Knowledge graph embedding has been an active research topic for knowledge base completion, with progressive improvement from the initial TransE, TransH, DistMult et al to the current state-of-the-art ConvE. ConvE uses 2D convolution over embeddings and multiple layers of nonlinear features to model knowledge graphs. The model can be efficiently trained and scalable to large knowledge graphs. However, there is no structure enforcement in the embedding space of ConvE. The recent graph convolutional network (GCN) provides another way of learning graph node embedding by successfully utilizing graph connectivity structure. In this work, we propose a novel end-to-end Structure-Aware Convolutional Network (SACN) that takes the benefit of GCN and ConvE together. SACN consists of an encoder of a weighted graph convolutional network (WGCN), and a decoder of a convolutional network called Conv-TransE. WGCN utilizes knowledge graph node structure, node attributes and edge relation types. It has learnable weights that adapt the amount of information from neighbors used in local aggregation, leading to more accurate embeddings of graph nodes. Node attributes in the graph are represented as additional nodes in the WGCN. The decoder Conv-TransE enables the state-of-the-art ConvE to be translational between entities and relations while keeps the same link prediction performance as ConvE. We demonstrate the effectiveness of the proposed SACN on standard FB15k-237 and WN18RR datasets, and it gives about 10\% relative improvement over the state-of-the-art ConvE in terms of HITS@1, HITS@3 and HITS@10.},
	publisher = {arXiv},
	author = {Shang, Chao and Tang, Yun and Huang, Jing and Bi, Jinbo and He, Xiaodong and Zhou, Bowen},
	month = nov,
	year = {2018},
	note = {arXiv:1811.04441 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\ZUJPTJTL\\Shang et al. - 2018 - End-to-end Structure-Aware Convolutional Networks .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\DRA3R5GP\\1811.html:text/html},
}

@inproceedings{mahdisoltani_yago3_2015,
	title = {{YAGO3}: {A} {Knowledge} {Base} from {Multilingual} {Wikipedias}},
	abstract = {We present YAGO3, an extension of the YAGO knowledge base that combines the information from the Wikipedias in multiple languages. Our technique fuses the multilingual information with the English WordNet to build one coherent knowledge base. We make use of the categories, the infoboxes, and Wikidata, and learn the meaning of infobox attributes across languages. We run our method on 10 different languages, and achieve a precision of 95\%-100\% in the attribute mapping. Our technique enlarges YAGO by 1m new entities and 7m new facts.},
	booktitle = {Proceedings of the 7th biennial conference on innovative data systems research},
	author = {Mahdisoltani, Farzaneh and Biega, Joanna and Suchanek, Fabian M},
	year = {2015},
	file = {Mahdisoltani et al. - YAGO3 A Knowledge Base from Multilingual Wikipedi.pdf:C\:\\Users\\larsj\\Zotero\\storage\\GLX7ZUAM\\Mahdisoltani et al. - YAGO3 A Knowledge Base from Multilingual Wikipedi.pdf:application/pdf},
}
