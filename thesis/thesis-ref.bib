
@article{meilicke_reinforced_2020,
	title = {Reinforced {Anytime} {Bottom} {Up} {Rule} {Learning} for {Knowledge} {Graph} {Completion}},
	url = {http://arxiv.org/abs/2004.04412},
	abstract = {Most of todays work on knowledge graph completion is concerned with sub-symbolic approaches that focus on the concept of embedding a given graph in a low dimensional vector space. Against this trend, we propose an approach called AnyBURL that is rooted in the symbolic space. Its core algorithm is based on sampling paths, which are generalized into Horn rules. Previously published results show that the prediction quality of AnyBURL is on the same level as current state of the art with the additional benefit of offering an explanation for the predicted fact. In this paper, we are concerned with two extensions of AnyBURL. Firstly, we change AnyBURLs interpretation of rules from \${\textbackslash}Theta\$-subsumption into \${\textbackslash}Theta\$-subsumption under Object Identity. Secondly, we introduce reinforcement learning to better guide the sampling process. We found out that reinforcement learning helps finding more valuable rules earlier in the search process. We measure the impact of both extensions and compare the resulting approach with current state of the art approaches. Our results show that AnyBURL outperforms most sub-symbolic methods.},
	urldate = {2022-01-28},
	journal = {arXiv:2004.04412 [cs]},
	author = {Meilicke, Christian and Chekol, Melisachew Wudage and Fink, Manuel and Stuckenschmidt, Heiner},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.04412},
	keywords = {\#todo},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\9CUA3UJU\\Meilicke et al. - 2020 - Reinforced Anytime Bottom Up Rule Learning for Kno.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\BP23X9VL\\2004.html:text/html},
}

@misc{meilicke_anyburl_2022,
	title = {{AnyBURL}},
	url = {https://web.informatik.uni-mannheim.de/AnyBURL/},
	urldate = {2022-01-28},
	author = {Meilicke, Christian},
	month = jan,
	year = {2022},
	keywords = {\#todo},
	file = {AnyBURL:C\:\\Users\\larsj\\Zotero\\storage\\APKG3PFJ\\AnyBURL.html:text/html},
}

@article{ott_safran_2021,
	title = {{SAFRAN}: {An} interpretable, rule-based link prediction method outperforming embedding models},
	shorttitle = {{SAFRAN}},
	url = {http://arxiv.org/abs/2109.08002},
	abstract = {Neural embedding-based machine learning models have shown promise for predicting novel links in knowledge graphs. Unfortunately, their practical utility is diminished by their lack of interpretability. Recently, the fully interpretable, rule-based algorithm AnyBURL yielded highly competitive results on many general-purpose link prediction benchmarks. However, current approaches for aggregating predictions made by multiple rules are affected by redundancies. We improve upon AnyBURL by introducing the SAFRAN rule application framework, which uses a novel aggregation approach called Non-redundant Noisy-OR that detects and clusters redundant rules prior to aggregation. SAFRAN yields new state-of-the-art results for fully interpretable link prediction on the established general-purpose benchmarks FB15K-237, WN18RR and YAGO3-10. Furthermore, it exceeds the results of multiple established embedding-based algorithms on FB15K-237 and WN18RR and narrows the gap between rule-based and embedding-based algorithms on YAGO3-10.},
	urldate = {2022-01-28},
	journal = {arXiv:2109.08002 [cs]},
	author = {Ott, Simon and Meilicke, Christian and Samwald, Matthias},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.08002},
	keywords = {\#todo},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\VJGH5TY3\\Ott et al. - 2021 - SAFRAN An interpretable, rule-based link predicti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\86XBDNEQ\\2109.html:text/html},
}

@misc{noauthor_uma-pi1kge_2022,
	title = {uma-pi1/kge},
	copyright = {MIT},
	url = {https://github.com/uma-pi1/kge},
	abstract = {LibKGE - A knowledge graph embedding library for reproducible research},
	urldate = {2022-01-28},
	publisher = {Chair of Data Analytics, University of Mannheim, Germany},
	month = jan,
	year = {2022},
	note = {original-date: 2019-02-22T18:39:36Z},
}

@inproceedings{meilicke_anytime_2019,
	address = {Macao, China},
	title = {Anytime {Bottom}-{Up} {Rule} {Learning} for {Knowledge} {Graph} {Completion}},
	isbn = {978-0-9992411-4-1},
	url = {https://www.ijcai.org/proceedings/2019/435},
	doi = {10.24963/ijcai.2019/435},
	abstract = {We propose an anytime bottom-up technique for learning logical rules from large knowledge graphs. We apply the learned rules to predict candidates in the context of knowledge graph completion. Our approach outperforms other rule-based approaches and it is competitive with current state of the art, which is based on latent representations. Besides, our approach is significantly faster, requires less computational resources, and yields an explanation in terms of the rules that propose a candidate.},
	language = {en},
	urldate = {2022-01-28},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Meilicke, Christian and Chekol, Melisachew Wudage and Ruffinelli, Daniel and Stuckenschmidt, Heiner},
	month = aug,
	year = {2019},
	keywords = {\#todo},
	pages = {3137--3143},
	file = {Meilicke et al. - 2019 - Anytime Bottom-Up Rule Learning for Knowledge Grap.pdf:C\:\\Users\\larsj\\Zotero\\storage\\XXW898RH\\Meilicke et al. - 2019 - Anytime Bottom-Up Rule Learning for Knowledge Grap.pdf:application/pdf},
}

@article{meilicke_why_nodate,
	title = {Why a {Naive} {Way} to {Combine} {Symbolic} and {Latent} {Knowledge} {Base} {Completion} {Works} {Surprisingly} {Well}},
	abstract = {We compare a rule-based approach for knowledge graph completion against current state-of-the-art, which is based on embeddings. Instead of focusing on aggregated metrics, we look at several examples that illustrate essential differences between symbolic and latent approaches. Based on our insights, we construct a simple method to combine the outcome of rule-based and latent approaches in a post-processing step. Our method improves the results constantly for each model and dataset used in our experiments.},
	language = {en},
	author = {Meilicke, Christian and Betz, Patrick and Stuckenschmidt, Heiner},
	keywords = {\#todo},
	pages = {26},
	file = {Meilicke et al. - Why a Naive Way to Combine Symbolic and Latent Kno.pdf:C\:\\Users\\larsj\\Zotero\\storage\\98EZQLT8\\Meilicke et al. - Why a Naive Way to Combine Symbolic and Latent Kno.pdf:application/pdf},
}

@inproceedings{socher_reasoning_2013,
	title = {Reasoning {With} {Neural} {Tensor} {Networks} for {Knowledge} {Base} {Completion}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/b337e84de8752b27eda3a12363109e80-Abstract.html},
	urldate = {2022-01-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Socher, Richard and Chen, Danqi and Manning, Christopher D and Ng, Andrew},
	year = {2013},
	file = {Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\TGCF4WEF\\Socher et al. - 2013 - Reasoning With Neural Tensor Networks for Knowledg.pdf:application/pdf},
}

@inproceedings{bordes_learning_2011,
	title = {Learning {Structured} {Embeddings} of {Knowledge} {Bases}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys{\textquoteright} fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author{\textquoteright}s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author{\textquoteright}s employer, and then only on the author{\textquoteright}s or the employer{\textquoteright}s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author{\textquoteright}s or the employer{\textquoteright}s creation (including tables of contents with links to other papers) without AAAI{\textquoteright}s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3659},
	abstract = {Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigorous symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like nat- ural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning meth- ods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text.},
	language = {en},
	urldate = {2022-01-28},
	booktitle = {Twenty-{Fifth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Bordes, Antoine and Weston, Jason and Collobert, Ronan and Bengio, Yoshua},
	month = aug,
	year = {2011},
	file = {Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\8JNI9V6R\\Bordes et al. - 2011 - Learning Structured Embeddings of Knowledge Bases.pdf:application/pdf;Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\UW7QSGHS\\3659.html:text/html},
}

@inproceedings{bordes_translating_2013,
	title = {Translating {Embeddings} for {Modeling} {Multi}-relational {Data}},
	volume = {26},
	url = {https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html},
	urldate = {2022-01-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
	year = {2013},
	file = {Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\EBUB82X5\\Bordes et al. - 2013 - Translating Embeddings for Modeling Multi-relation.pdf:application/pdf},
}

@inproceedings{ji_knowledge_2015,
	address = {Beijing, China},
	title = {Knowledge {Graph} {Embedding} via {Dynamic} {Mapping} {Matrix}},
	url = {https://aclanthology.org/P15-1067},
	doi = {10.3115/v1/P15-1067},
	urldate = {2022-01-28},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ji, Guoliang and He, Shizhu and Xu, Liheng and Liu, Kang and Zhao, Jun},
	month = jul,
	year = {2015},
	pages = {687--696},
	file = {Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\SVU8WUKN\\Ji et al. - 2015 - Knowledge Graph Embedding via Dynamic Mapping Matr.pdf:application/pdf},
}

@book{fensel_introduction_2020,
	address = {Cham},
	title = {Introduction: {What} {Is} a {Knowledge} {Graph}?},
	isbn = {978-3-030-37439-6},
	shorttitle = {Introduction},
	url = {https://doi.org/10.1007/978-3-030-37439-6_1},
	abstract = {Since its inception by Google, Knowledge Graph has become a term that is recently ubiquitously used yet does not have a well-established definition. This section attempts to derive a definition for Knowledge Graphs by compiling existing definitions made in the literature and considering the distinctive characteristics of previous efforts for tackling the data integration challenge we are facing today. Our attempt to make a conceptual definition is complemented with an empirical survey of existing Knowledge Graphs. This section lays the foundation for the remainder of the book, as it provides a common understanding on certain concepts and motivation to build Knowledge Graphs in the first place.},
	language = {en},
	urldate = {2022-02-03},
	publisher = {Springer International Publishing},
	author = {Fensel, Dieter and {\c S}im{\c s}ek, Umutcan and Angele, Kevin and Huaman, Elwin and K{\"a}rle, Elias and Panasiuk, Oleksandra and Toma, Ioan and Umbrich, J{\"u}rgen and Wahler, Alexander},
	editor = {Fensel, Dieter and {\c S}im{\c s}ek, Umutcan and Angele, Kevin and Huaman, Elwin and K{\"a}rle, Elias and Panasiuk, Oleksandra and Toma, Ioan and Umbrich, J{\"u}rgen and Wahler, Alexander},
	year = {2020},
	doi = {10.1007/978-3-030-37439-6_1},
}

@article{trouillon_complex_nodate,
	title = {Complex {Embeddings} for {Simple} {Link} {Prediction}},
	language = {en},
	author = {Trouillon, Theo and Welbl, Johannes and Riedel, Sebastian},
	pages = {10},
	file = {Trouillon et al. - Complex Embeddings for Simple Link Prediction.pdf:C\:\\Users\\larsj\\Zotero\\storage\\AKYKIUGK\\Trouillon et al. - Complex Embeddings for Simple Link Prediction.pdf:application/pdf},
}

@article{nickel_three-way_nodate,
	title = {A {Three}-{Way} {Model} for {Collective} {Learning} on {Multi}-{Relational} {Data}},
	abstract = {Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.},
	language = {en},
	author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
	pages = {8},
	file = {Nickel et al. - A Three-Way Model for Collective Learning on Multi.pdf:C\:\\Users\\larsj\\Zotero\\storage\\4BABBAPD\\Nickel et al. - A Three-Way Model for Collective Learning on Multi.pdf:application/pdf},
}

@book{milton_knowledge_2007,
	title = {Knowledge {Acquisition} in {Practice}: {A} {Step}-by-step {Guide}},
	isbn = {978-1-84628-861-6},
	shorttitle = {Knowledge {Acquisition} in {Practice}},
	abstract = {Recent years have seen an upsurge of interest in knowledge. Leading organisations now recognise the importance of identifying what they know, sharing what they know and using what they know for maximum benefit. Many organisations employ knowledge engineers to capture knowledge from experts using the principles and techniques of knowledge engineering. The emphasis is on a structured approach built on a sound understanding of the psychology of expertise and making use of knowledge modelling methods and the latest web technologies.   Knowledge Acquisition in Practice is the first book to provide a detailed step-by-step guide to the methods and practical aspects of acquiring, modelling, storing and sharing knowledge. The reader is led through 47 steps from the inception of a project to its successful conclusion. Each step is described in terms of the reasons for the step, the required resources, the activities to be undertaken, and the solutions to common problems. In addition, each step has a checklist which lists the key items that should be achieved during the step.  Knowledge Acquisition in Practice will be of value to knowledge engineers, knowledge workers, knowledge officers and ontological engineers. The book will also be of interest to students and researchers of AI, computer science and business studies.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Milton, Nicholas Ross},
	month = may,
	year = {2007},
	keywords = {Business \& Economics / Business Mathematics, Business \& Economics / Information Management, Computers / Artificial Intelligence / General, Computers / Information Technology, Language Arts \& Disciplines / Library \& Information Science / General, Technology \& Engineering / Engineering (General), Technology \& Engineering / General},
}

@article{paulheim_automatic_nodate,
	title = {Automatic {Knowledge} {Graph} {Refinement}: {A} {Survey} of {Approaches} and {Evaluation} {Methods}},
	abstract = {In the recent years, different web knowledge graphs, both free and commercial, have been created, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and NLP methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.},
	language = {en},
	author = {Paulheim, Heiko},
	pages = {17},
	file = {Paulheim - Automatic Knowledge Graph Refinement A Survey of A.pdf:C\:\\Users\\larsj\\Zotero\\storage\\BYHNW9JP\\Paulheim - Automatic Knowledge Graph Refinement A Survey of A.pdf:application/pdf},
}

@article{paulheim_knowledge_2016,
	title = {Knowledge graph refinement: {A} survey of approaches and evaluation methods},
	volume = {8},
	issn = {22104968, 15700844},
	shorttitle = {Knowledge graph refinement},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-160218},
	doi = {10.3233/SW-160218},
	abstract = {In the recent years, different Web knowledge graphs, both free and commercial, have been created. While Google coined the term {\textquotedblleft}Knowledge Graph{\textquotedblright} in 2012, there are also a few openly available knowledge graphs, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and linguistic methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of such knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.},
	language = {en},
	number = {3},
	urldate = {2022-05-11},
	journal = {Semantic Web},
	author = {Paulheim, Heiko},
	editor = {Cimiano, Philipp},
	month = dec,
	year = {2016},
	pages = {489--508},
	file = {Paulheim - 2016 - Knowledge graph refinement A survey of approaches.pdf:C\:\\Users\\larsj\\Zotero\\storage\\4HCUHBWU\\Paulheim - 2016 - Knowledge graph refinement A survey of approaches.pdf:application/pdf},
}

@book{talburt_entity_2011,
	title = {Entity {Resolution} and {Information} {Quality}},
	isbn = {978-0-12-381973-4},
	abstract = {Entity Resolution and Information Quality presents topics and definitions, and clarifies confusing terminologies regarding entity resolution and information quality. It takes a very wide view of IQ, including its six-domain framework and the skills formed by the International Association for Information and Data Quality \{\vphantom{\}}IAIDQ). The book includes chapters that cover the principles of entity resolution and the principles of Information Quality, in addition to their concepts and terminology. It also discusses the Fellegi-Sunter theory of record linkage, the Stanford Entity Resolution Framework, and the Algebraic Model for Entity Resolution, which are the major theoretical models that support Entity Resolution. In relation to this, the book briefly discusses entity-based data integration (EBDI) and its model, which serve as an extension of the Algebraic Model for Entity Resolution. There is also an explanation of how the three commercial ER systems operate and a description of the non-commercial open-source system known as OYSTER. The book concludes by discussing trends in entity resolution research and practice. Students taking IT courses and IT professionals will find this book invaluable.First authoritative reference explaining entity resolution and how to use it effectivelyProvides practical system design advice to help you get a competitive advantage Includes a companion site with synthetic customer data for applicatory exercises, and access to a Java-based Entity Resolution program.},
	language = {en},
	publisher = {Elsevier},
	author = {Talburt, John},
	month = jan,
	year = {2011},
	note = {Google-Books-ID: tIB0IZYR8V8C},
	keywords = {Business \& Economics / Sales \& Selling / General, Computers / Business \& Productivity Software / Business Intelligence, Computers / Business \& Productivity Software / General, Computers / Database Administration \& Management, Computers / Management Information Systems},
}

@inproceedings{baumgartner_entity_2021,
	address = {Mexico City, Mexico},
	title = {Entity {Prediction} in {Knowledge} {Graphs} with {Joint} {Embeddings}},
	url = {https://aclanthology.org/2021.textgraphs-1.3},
	doi = {10.18653/v1/2021.textgraphs-1.3},
	abstract = {Knowledge Graphs (KGs) have become increasingly popular in the recent years. However, as knowledge constantly grows and changes, it is inevitable to extend existing KGs with entities that emerged or became relevant to the scope of the KG after its creation. Research on updating KGs typically relies on extracting named entities and relations from text. However, these approaches cannot infer entities or relations that were not explicitly stated. Alternatively, embedding models exploit implicit structural regularities to predict missing relations, but cannot predict missing entities. In this article, we introduce a novel method to enrich a KG with new entities given their textual description. Our method leverages joint embedding models, hence does not require entities or relations to be named explicitly. We show that our approach can identify new concepts in a document corpus and transfer them into the KG, and we find that the performance of our method improves substantially when extended with techniques from association rule mining, text mining, and active learning.},
	urldate = {2022-05-11},
	booktitle = {Proceedings of the {Fifteenth} {Workshop} on {Graph}-{Based} {Methods} for {Natural} {Language} {Processing} ({TextGraphs}-15)},
	publisher = {Association for Computational Linguistics},
	author = {Baumgartner, Matthias and Dell'Aglio, Daniele and Bernstein, Abraham},
	month = jun,
	year = {2021},
	pages = {22--31},
	file = {Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\INJIVYJ8\\Baumgartner et al. - 2021 - Entity Prediction in Knowledge Graphs with Joint E.pdf:application/pdf},
}

@book{golbeck_analyzing_2013,
	title = {Analyzing the {Social} {Web}},
	isbn = {978-0-12-405856-9},
	abstract = {Analyzing the Social Web provides a framework for the analysis of public data currently available and being generated by social networks and social media, like Facebook, Twitter, and Foursquare. Access and analysis of this public data about people and their connections to one another allows for new applications of traditional social network analysis techniques that let us identify things like who are the most important or influential people in a network, how things will spread through the network, and the nature of peoples' relationships. Analyzing the Social Web introduces you to these techniques, shows you their application to many different types of social media, and discusses how social media can be used as a tool for interacting with the online public. Presents interactive social applications on the web, and the types of analysis that are currently conducted in the study of social media Covers the basics of network structures for beginners, including measuring methods for describing nodes, edges, and parts of the network Discusses the major categories of social media applications or phenomena and shows how the techniques presented can be applied to analyze and understand the underlying data Provides an introduction to information visualization, particularly network visualization techniques, and methods for using them to identify interesting features in a network, generate hypotheses for analysis, and recognize patterns of behavior Includes a supporting website with lecture slides, exercises, and downloadable social network data sets that can be used can be used to apply the techniques presented in the book},
	language = {en},
	publisher = {Newnes},
	author = {Golbeck, Jennifer},
	month = feb,
	year = {2013},
	note = {Google-Books-ID: XP8jc2cDNrwC},
	keywords = {Computers / Human-Computer Interaction (HCI), Computers / Internet / General},
}

@book{ilkou_symbolic_2020,
	title = {Symbolic {Vs} {Sub}-symbolic {AI} {Methods}: {Friends} or {Enemies}?},
	shorttitle = {Symbolic {Vs} {Sub}-symbolic {AI} {Methods}},
	abstract = {There is a long and unresolved debate between the symbolic and sub-symbolic methods. However, in recent years, there is a push towards in-between methods. In this work, we provide a comprehensive overview of the symbolic, sub-symbolic and in-between approaches focused in the domain of knowledge graphs, namely, schema representation, schema matching, knowledge graph completion, link prediction, entity resolution, entity classification and triple classification. We critically present key characteristics, advantages and disadvantages of the main algorithms in each domain, and review the use of these methods in knowledge graph related applications.},
	author = {Ilkou, Eleni and Koutraki, Maria},
	month = nov,
	year = {2020},
	doi = {10.1145/3340531.3414072},
}

@article{hatzilygeroudis_neuro-symbolic_2005,
	title = {Neuro-{Symbolic} {Approaches} for {Knowledge} {Representation} in {Expert} {Systems}},
	volume = {1},
	issn = {18758819, 14485869},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/HIS-2004-13-401},
	doi = {10.3233/HIS-2004-13-401},
	abstract = {In this paper, we first present and compare existing categorization schemes for neuro-symbolic approaches. We then stress the point that not all hybrid neuro-symbolic approaches can be accommodated by existing categories. Such a case is rule-based neuro-symbolic approaches that propose a unified knowledge representation scheme suitable for use in expert systems. That kind of integrated schemes have the two component approaches tightly and indistinguishably integrated, offer an interactive inference engine and can provide explanations. Therefore, we introduce a new category of neuro-symbolic integrations, namely {\textquoteleft}representational integrations{\textquoteright}. Furthermore, two sub-categories of representational integrations are distinguished, based on which of the two component approaches of the integrations is given pre-eminence. Representative approaches as well as advantages and disadvantages of both sub-categories are discussed.},
	language = {en},
	number = {3-4},
	urldate = {2022-05-17},
	journal = {International Journal of Hybrid Intelligent Systems},
	author = {Hatzilygeroudis, Ioannis and Prentzas, Jim},
	month = jan,
	year = {2005},
	pages = {111--126},
	file = {Hatzilygeroudis and Prentzas - 2005 - Neuro-Symbolic Approaches for Knowledge Representa.pdf:C\:\\Users\\larsj\\Zotero\\storage\\9B93VMFQ\\Hatzilygeroudis and Prentzas - 2005 - Neuro-Symbolic Approaches for Knowledge Representa.pdf:application/pdf},
}

@article{nickel_review_2015,
	title = {A {Review} of {Relational} {Machine} {Learning} for {Knowledge} {Graphs}},
	volume = {104},
	issn = {0018-9219, 1558-2256},
	url = {http://arxiv.org/abs/1503.00759},
	doi = {10.1109/JPROC.2015.2483592},
	abstract = {Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be "trained" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.},
	number = {1},
	urldate = {2022-05-17},
	journal = {Proceedings of the IEEE},
	author = {Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy},
	year = {2015},
	note = {arXiv:1503.00759 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {11--33},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\2GX972B6\\Nickel et al. - 2016 - A Review of Relational Machine Learning for Knowle.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\65NHX499\\1503.html:text/html},
}

@article{zou_survey_2020,
	title = {A {Survey} on {Application} of {Knowledge} {Graph}},
	volume = {1487},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1487/1/012016},
	doi = {10.1088/1742-6596/1487/1/012016},
	abstract = {Knowledge graphs, representation of information as a semantic graph, have caused wide concern in both industrial and academic world. Their property of providing semantically structured information has brought important possible solutions for many tasks including question answering, recommendation and information retrieval, and is considered to offer great promise for building more intelligent machines by many researchers. Although knowledge graphs have already supported multiple {\textquotedblleft}Big Data{\textquotedblright} applications in all sorts of commercial and scientific domains since Google coined this term in 2012, there was no previous study give a systemically review of the application of knowledge graphs. Therefore, unlike other related work which focuses on the construction techniques of knowledge graphs, this present paper aims at providing a first survey on these applications stemming from different domains. This paper also points out that while important advancements of applying knowledge graphs' great ability of providing semantically structured information into specific domains have been made in recent years, several aspects still remain to be explored.},
	language = {en},
	number = {1},
	urldate = {2022-05-17},
	journal = {Journal of Physics: Conference Series},
	author = {Zou, Xiaohan},
	month = mar,
	year = {2020},
	pages = {012016},
	file = {Zou - 2020 - A Survey on Application of Knowledge Graph.pdf:C\:\\Users\\larsj\\Zotero\\storage\\D5CTM4W7\\Zou - 2020 - A Survey on Application of Knowledge Graph.pdf:application/pdf},
}

@misc{singhal_introducing_2012,
	title = {Introducing the {Knowledge} {Graph}: things, not strings},
	shorttitle = {Introducing the {Knowledge} {Graph}},
	url = {https://blog.google/products/search/introducing-knowledge-graph-things-not/},
	abstract = {We hope this will give you a more complete picture of your interest, provide smarter search results, and pique your curiosity.},
	language = {en-us},
	urldate = {2022-05-17},
	journal = {Google},
	author = {Singhal, Amit},
	month = may,
	year = {2012},
	file = {Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\F3CF9BWH\\introducing-knowledge-graph-things-not.html:text/html},
}

@techreport{bianchi_knowledge_2020,
	title = {Knowledge {Graph} {Embeddings} and {Explainable} {AI}},
	url = {http://arxiv.org/abs/2004.14843},
	abstract = {Knowledge graph embeddings are now a widely adopted approach to knowledge representation in which entities and relationships are embedded in vector spaces. In this chapter, we introduce the reader to the concept of knowledge graph embeddings by explaining what they are, how they can be generated and how they can be evaluated. We summarize the state-of-the-art in this field by describing the approaches that have been introduced to represent knowledge in the vector space. In relation to knowledge representation, we consider the problem of explainability, and discuss models and methods for explaining predictions obtained via knowledge graph embeddings.},
	urldate = {2022-05-19},
	author = {Bianchi, Federico and Rossiello, Gaetano and Costabello, Luca and Palmonari, Matteo and Minervini, Pasquale},
	month = apr,
	year = {2020},
	doi = {10.3233/SSW200011},
	note = {arXiv:2004.14843 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\QD83KNMR\\Bianchi et al. - 2020 - Knowledge Graph Embeddings and Explainable AI.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\EXTYT2GP\\2004.html:text/html},
}

@article{trouillon_complex_2016,
	title = {Complex {Embeddings} for {Simple} {Link} {Prediction}},
	language = {en},
	author = {Trouillon, Theo and Welbl, Johannes and Riedel, Sebastian},
	year = {2016},
	pages = {10},
	file = {Trouillon et al. - Complex Embeddings for Simple Link Prediction.pdf:C\:\\Users\\larsj\\Zotero\\storage\\CMD2JDD7\\Trouillon et al. - Complex Embeddings for Simple Link Prediction.pdf:application/pdf},
}

@article{nickel_three-way_2011,
	title = {A {Three}-{Way} {Model} for {Collective} {Learning} on {Multi}-{Relational} {Data}},
	abstract = {Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.},
	language = {en},
	author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
	year = {2011},
	pages = {8},
	file = {Nickel et al. - A Three-Way Model for Collective Learning on Multi.pdf:C\:\\Users\\larsj\\Zotero\\storage\\7RAJQLMF\\Nickel et al. - A Three-Way Model for Collective Learning on Multi.pdf:application/pdf},
}

@article{krompas_non-negative_nodate,
	title = {Non-{Negative} {Tensor} {Factorization} with},
	abstract = {Non-negative data is generated by a broad selection of applications today, e.g in gene expression analysis or imaging. Many factorization techniques have been extended to account for this natural constraint and have become very popular due to their decomposition into interpretable latent factors. Generally relational data like protein interaction networks or social network data can also be seen as naturally non-negative. In this work, we extend the RESCAL tensor factorization, which has shown state-of-the-art results for multi-relational learning, to account for non-negativity by employing multiplicative update rules. We study the performance via these approaches on various benchmark datasets and show that a non-negativity constraint can be introduced by losing only little in terms of predictive quality in most of the cases but simultaneously increasing the sparsity of the factors significantly compared to the original RESCAL algorithm.},
	language = {en},
	author = {Krompa{\ss}, Denis and Nickel, Maximilian and Jiang, Xueyan and Tresp, Volker},
	pages = {10},
	file = {Krompa{\ss} et al. - Non-Negative Tensor Factorization with.pdf:C\:\\Users\\larsj\\Zotero\\storage\\XVRBH8QE\\Krompa{\ss} et al. - Non-Negative Tensor Factorization with.pdf:application/pdf},
}

@misc{safavi_codex_2020,
	title = {{CoDEx}: {A} {Comprehensive} {Knowledge} {Graph} {Completion} {Benchmark}},
	shorttitle = {{CoDEx}},
	url = {http://arxiv.org/abs/2009.07810},
	abstract = {We present CODEX, a set of knowledge graph COmpletion Datasets EXtracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CODEX comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. To characterize CODEX, we contribute thorough empirical analyses and benchmarking experiments. First, we analyze each CODEX dataset in terms of logical relation patterns. Next, we report baseline link prediction and triple classification results on CODEX for five extensively tuned embedding models. Finally, we differentiate CODEX from the popular FB15K-237 knowledge graph completion dataset by showing that CODEX covers more diverse and interpretable content, and is a more difficult link prediction benchmark. Data, code, and pretrained models are available at https://bit.ly/2EPbrJs.},
	language = {en},
	urldate = {2022-05-25},
	publisher = {arXiv},
	author = {Safavi, Tara and Koutra, Danai},
	month = oct,
	year = {2020},
	note = {Number: arXiv:2009.07810
arXiv:2009.07810 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Safavi and Koutra - 2020 - CoDEx A Comprehensive Knowledge Graph Completion .pdf:C\:\\Users\\larsj\\Zotero\\storage\\JHJF6KTY\\Safavi and Koutra - 2020 - CoDEx A Comprehensive Knowledge Graph Completion .pdf:application/pdf},
}

@article{chen_knowledge_2020,
	title = {Knowledge {Graph} {Completion}: {A} {Review}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Knowledge {Graph} {Completion}},
	doi = {10.1109/ACCESS.2020.3030076},
	abstract = {Knowledge graph completion (KGC) is a hot topic in knowledge graph construction and related applications, which aims to complete the structure of knowledge graph by predicting the missing entities or relationships in knowledge graph and mining unknown facts. Starting from the definition and types of KGC, existing technologies for KGC are analyzed in categories. From the evolving point of view, the KGC technologies could be divided into traditional and representation learning based methods. The former mainly includes rule-based reasoning method, probability graph model, such as Markov logic network, and graph computation based method. The latter further includes translation model based, semantic matching model based, representation learning based and other neural network model based methods. In this article, different KGC technologies are introduced, including their advantages, disadvantages and applicable fields. Finally the main challenges and problems faced by the KGC are discussed, as well as the potential research directions.},
	journal = {IEEE Access},
	author = {Chen, Zhe and Wang, Yuehan and Zhao, Bin and Cheng, Jing and Zhao, Xin and Duan, Zongtao},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Semantics, Cognition, Computational modeling, deep learning, entity prediction, Knowledge graph, knowledge graph completion, Knowledge representation, Natural language processing, relation prediction, Task analysis},
	pages = {192435--192456},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\larsj\\Zotero\\storage\\VVT8ADK9\\9220143.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\larsj\\Zotero\\storage\\LNGTIVYG\\Chen et al. - 2020 - Knowledge Graph Completion A Review.pdf:application/pdf},
}

@techreport{yang_embedding_2015,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	url = {http://arxiv.org/abs/1412.6575},
	abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) ={\textgreater} Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
	number = {arXiv:1412.6575},
	urldate = {2022-06-01},
	institution = {arXiv},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	month = aug,
	year = {2015},
	note = {arXiv:1412.6575 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\ZWRYCTK4\\Yang et al. - 2015 - Embedding Entities and Relations for Learning and .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\EHNX893D\\1412.html:text/html},
}

@inproceedings{nickel_factorizing_2012,
	address = {Lyon France},
	title = {Factorizing {YAGO}: scalable machine learning for linked data},
	isbn = {978-1-4503-1229-5},
	shorttitle = {Factorizing {YAGO}},
	url = {https://dl.acm.org/doi/10.1145/2187836.2187874},
	doi = {10.1145/2187836.2187874},
	abstract = {Vast amounts of structured information have been published in the Semantic Web{\textquoteright}s Linked Open Data (LOD) cloud and their size is still growing rapidly. Yet, access to this information via reasoning and querying is sometimes difficult, due to LOD{\textquoteright}s size, partial data inconsistencies and inherent noisiness. Machine Learning offers an alternative approach to exploiting LOD{\textquoteright}s data with the advantages that Machine Learning algorithms are typically robust to both noise and data inconsistencies and are able to efficiently utilize nondeterministic dependencies in the data. From a Machine Learning point of view, LOD is challenging due to its relational nature and its scale. Here, we present an efficient approach to relational learning on LOD data, based on the factorization of a sparse tensor that scales to data consisting of millions of entities, hundreds of relations and billions of known facts. Furthermore, we show how ontological knowledge can be incorporated in the factorization to improve learning results and how computation can be distributed across multiple nodes. We demonstrate that our approach is able to factorize the YAGO 2 core ontology and globally predict statements for this large knowledge base using a single dual-core desktop computer. Furthermore, we show experimentally that our approach achieves good results in several relational learning tasks that are relevant to Linked Data. Once a factorization has been computed, our model is able to predict efficiently, and without any additional training, the likelihood of any of the 4.3 {\textperiodcentered} 1014 possible triples in the YAGO 2 core ontology.},
	language = {en},
	urldate = {2022-06-01},
	booktitle = {Proceedings of the 21st international conference on {World} {Wide} {Web}},
	publisher = {ACM},
	author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
	month = apr,
	year = {2012},
	pages = {271--280},
	file = {Nickel et al. - 2012 - Factorizing YAGO scalable machine learning for li.pdf:C\:\\Users\\larsj\\Zotero\\storage\\SCMQ4ZT7\\Nickel et al. - 2012 - Factorizing YAGO scalable machine learning for li.pdf:application/pdf},
}

@inproceedings{nickel_factorizing_2012-1,
	address = {Lyon France},
	title = {Factorizing {YAGO}: scalable machine learning for linked data},
	isbn = {978-1-4503-1229-5},
	shorttitle = {Factorizing {YAGO}},
	url = {https://dl.acm.org/doi/10.1145/2187836.2187874},
	doi = {10.1145/2187836.2187874},
	abstract = {Vast amounts of structured information have been published in the Semantic Web{\textquoteright}s Linked Open Data (LOD) cloud and their size is still growing rapidly. Yet, access to this information via reasoning and querying is sometimes difficult, due to LOD{\textquoteright}s size, partial data inconsistencies and inherent noisiness. Machine Learning offers an alternative approach to exploiting LOD{\textquoteright}s data with the advantages that Machine Learning algorithms are typically robust to both noise and data inconsistencies and are able to efficiently utilize nondeterministic dependencies in the data. From a Machine Learning point of view, LOD is challenging due to its relational nature and its scale. Here, we present an efficient approach to relational learning on LOD data, based on the factorization of a sparse tensor that scales to data consisting of millions of entities, hundreds of relations and billions of known facts. Furthermore, we show how ontological knowledge can be incorporated in the factorization to improve learning results and how computation can be distributed across multiple nodes. We demonstrate that our approach is able to factorize the YAGO 2 core ontology and globally predict statements for this large knowledge base using a single dual-core desktop computer. Furthermore, we show experimentally that our approach achieves good results in several relational learning tasks that are relevant to Linked Data. Once a factorization has been computed, our model is able to predict efficiently, and without any additional training, the likelihood of any of the 4.3 {\textperiodcentered} 1014 possible triples in the YAGO 2 core ontology.},
	language = {en},
	urldate = {2022-06-01},
	booktitle = {Proceedings of the 21st international conference on {World} {Wide} {Web}},
	publisher = {ACM},
	author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
	month = apr,
	year = {2012},
	pages = {271--280},
	file = {Nickel et al. - 2012 - Factorizing YAGO scalable machine learning for li.pdf:C\:\\Users\\larsj\\Zotero\\storage\\6SWGRIG7\\Nickel et al. - 2012 - Factorizing YAGO scalable machine learning for li.pdf:application/pdf},
}

@techreport{yang_embedding_2015-1,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	url = {http://arxiv.org/abs/1412.6575},
	abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) ={\textgreater} Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
	number = {arXiv:1412.6575},
	urldate = {2022-06-01},
	institution = {arXiv},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	month = aug,
	year = {2015},
	note = {arXiv:1412.6575 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\JIWMHLTU\\Yang et al. - 2015 - Embedding Entities and Relations for Learning and .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\NAXB6TRS\\1412.html:text/html},
}

@techreport{yang_embedding_2015-2,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	url = {http://arxiv.org/abs/1412.6575},
	abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) ={\textgreater} Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
	number = {arXiv:1412.6575},
	urldate = {2022-06-01},
	institution = {arXiv},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	month = aug,
	year = {2015},
	note = {arXiv:1412.6575 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\larsj\\Zotero\\storage\\MJRE6UWX\\Yang et al. - 2015 - Embedding Entities and Relations for Learning and .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\larsj\\Zotero\\storage\\LYIV9IJR\\1412.html:text/html},
}
